{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alpha-composition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n",
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-hypothesis",
   "metadata": {},
   "source": [
    "# Derivatives of Gaussian KL-Divergence for some parameterizations of the posterior covariance for variational Gaussian-process inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-companion",
   "metadata": {},
   "source": [
    "> *These notes provide the derivatives of the KL-divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| P(\\mathbf z)\\right]$ between two multivariate Gaussian distributions $Q(\\mathbf z)$ and $P(\\mathbf z)$ with respect to a few parameterizations $\\theta$ of the covariance matrix $\\boldsymbol\\Sigma(\\theta)$ of $Q$. This is useful for variational Gaussian process inference, where clever parameterizations of the posterior covariance are required to make the problem tractable. Tables for differentiating matrix-valued functions can be found in [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-plumbing",
   "metadata": {},
   "source": [
    "Consider two multivariate Gaussian distributions $Q(\\mathbf z) = \\mathcal N(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma(\\theta))$ and $P(\\mathbf z) = \\mathcal N(\\boldsymbol\\mu_0,\\boldsymbol\\Sigma_0 = \\boldsymbol\\Lambda^{-1})$ with dimension $L$. The KL divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| P(\\mathbf z)\\right]$ [has the closed form](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\mathcal D \n",
    ":= &\\, D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]\n",
    "\\\\\n",
    "= &\\,\n",
    "\\tfrac 1 2 \\left\\{\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)^\\top \n",
    "\\boldsymbol\\Lambda\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)\n",
    "\\right.\n",
    "\\\\\n",
    "&\\,\\left.+\n",
    "\\operatorname{tr}\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\boldsymbol\\Sigma\n",
    "\\right)\n",
    "-\n",
    "\\ln|\\boldsymbol\\Sigma|\n",
    "-\n",
    "\\ln|\\boldsymbol\\Lambda|\n",
    "\\right\\}\n",
    "+\\text{constant.}\n",
    "\\end{aligned}\n",
    "\\label{dkl}\n",
    "\\end{equation}\n",
    "\n",
    "In variational Bayesian inference, we minimize $\\mathcal D$ while maximizing the expected log-probability of some observations with respect to $Q(\\mathbf z)$. Closed-form derivatives of $\\mathcal D$ in terms of the parameters of $Q$ are useful for manually optimizing code for larger problems. The derivatives of $\\mathcal D$ in terms of $\\boldsymbol\\mu_q$ are straightforward: $\\partial_{\\boldsymbol\\mu_q}\\mathcal D=\\boldsymbol\\Lambda(\\boldsymbol\\mu_q-\\boldsymbol\\mu_z)$\n",
    "and \n",
    "$\\operatorname H_{\\boldsymbol\\mu_q}\\mathcal D=\\boldsymbol\\Lambda$. In these notes, we explore derivatives of $\\mathcal D$ with respect to a few different parameterizations (\"$\\theta$\") of $\\boldsymbol\\Sigma(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-gothic",
   "metadata": {},
   "source": [
    "We evaluate the following parameterizations for $\\boldsymbol\\Sigma$: \n",
    "1. Optimizing the full $\\boldsymbol\\Sigma$ directly\n",
    "2. $\\boldsymbol\\Sigma\\approx\\mathbf X\\mathbf X^\\top$\n",
    "3. $\\boldsymbol\\Sigma\\approx\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$\n",
    "4. $\\boldsymbol\\Sigma\\approx[\\boldsymbol\\Lambda + \\operatorname{diag}[\\mathbf p]]^{-1}$\n",
    "5. $\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$, where $\\mathbf Q\\in\\mathbb R^{K{\\times}K}$, $K{<}L$ and $\\mathbf F\\in\\mathbb R^{K{\\times}L}$, \n",
    "$\\mathbf F\\mathbf F^\\top = \\mathbf I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-vacation",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}$\n",
    "\n",
    "We first obtain gradients of $\\mathcal D$ in ${\\boldsymbol\\Sigma}$ (assuming ${\\boldsymbol\\Sigma}$ is full-rank). These can be used to derive gradients in $\\theta$ for some parameterizations ${\\boldsymbol\\Sigma}(\\theta)$ using the chain rule. The gradient of $\\mathcal D$ in $\\boldsymbol\\Sigma$ can be obtained using identities (57) and (100) in The Matrix Cookbook:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "&=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\left\\{\n",
    "\\operatorname{tr}\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\boldsymbol\\Sigma\n",
    "\\right)\n",
    "-\n",
    "\\ln|\\boldsymbol\\Sigma|\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right).\n",
    "\\end{aligned}\\label{js}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-honduras",
   "metadata": {},
   "source": [
    "The Hessian in $\\boldsymbol\\Sigma$ is a fourth-order tensor. It's simpler to express the Hessian in terms of a Hessian-vector product, which can be used with [Krylov subspace](https://en.wikipedia.org/wiki/Krylov_subspace) solvers to efficiently compute the update in Newton's method. Considering an $L{\\times}L$ matrix $\\mathbf M$, the Hessian-vector product is given by \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\left[\n",
    "\\mathbf H_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "\\right] \\mathbf M\n",
    "&=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\left< \\partial_{\\boldsymbol\\Sigma}\\mathcal D,\\mathbf M\\right>\n",
    "=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "(\\partial_{\\boldsymbol\\Sigma}\\mathcal D)^\\top \\mathbf M\\right],\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-victoria",
   "metadata": {},
   "source": [
    "where $\\langle\\cdot,\\cdot\\rangle$ denotes the scalar (Frobenius) product. This is given by identity (124) in the Matrix Cookbook:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "\\frac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "^\\top\n",
    "\\mathbf M\n",
    "\\right]\n",
    "&=\n",
    "-\\frac 1 2 \n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\mathbf M\n",
    "\\right]\n",
    "=\n",
    "\\frac 1 2 \n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\mathbf M^\\top\n",
    "\\boldsymbol\\Sigma^{-1}.\n",
    "\\end{aligned}\\label{hvs}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-quick",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}{\\approx}{\\mathbf X}{{\\mathbf X}^{\\top}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-wonder",
   "metadata": {},
   "source": [
    "We consider an approximate posterior covariance of the form\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\boldsymbol\\Sigma &\\approx \\mathbf X \\mathbf X^\\top,\\,\\,\\,\\,\\mathbf X\\in\\mathbb R^{L\\times K}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-concentration",
   "metadata": {},
   "source": [
    "where $\\mathbf X$ is a rank-$K<L$ matrix with $L$ rows and $K$ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-surprise",
   "metadata": {},
   "source": [
    "Since $\\mathbf X$ is not full rank, the log-determinant $\\ln|{\\boldsymbol\\Sigma}|=\\ln|\\mathbf X\\mathbf X^\\top|$ in $\\eqref{dkl}$ diverges, due to the zero eigenvalues in the null space of $\\mathbf X$. However, since this null-space is not being optimized, it does not affect our gradient. It is sufficient to replace the log-determinant with that of the reduced-rank representation, $\\ln|\\mathbf X^\\top\\mathbf X|$. Identity (55) in The Matrix Cookbook provides the derivative of this, $\\partial_{\\mathbf X}\\ln|\\mathbf X^\\top\\mathbf X| = 2 {\\mathbf X^{+}}^\\top$, where $(\\cdot)^+$  is the pseudoinverse. Combined with identity (112), this gives the following gradient of $\\mathcal D(\\mathbf X)$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D &=\n",
    "\\partial_{\\mathbf X}\n",
    "\\tfrac 1 2 \\left\\{\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X\\mathbf X^\\top\n",
    "\\right]\n",
    "-\n",
    "\\ln|\\mathbf X^\\top\\mathbf X|\n",
    "\\right\\}\n",
    "=\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "-\n",
    "{\\mathbf X^{+}}^\\top.\n",
    "\\end{aligned}\n",
    "\\label{jxx}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-bachelor",
   "metadata": {},
   "source": [
    "The Hessian-vector product requires the derivative of $\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "-\n",
    "{\\mathbf X^{+}}^\\top\n",
    "\\right)\n",
    "^\\top\n",
    "\\mathbf M\n",
    "\\right]\n",
    "=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "\\mathbf M\n",
    "\\right]\n",
    "-\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "{\\mathbf X^{+}}\n",
    "\\mathbf M\n",
    "\\right].\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-polyester",
   "metadata": {},
   "source": [
    "Goulob and Pereya (1972) Eq. 4.12 gives the derivative of a fixed-rank pseudoinverse: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial \\mathbf X^+ =\n",
    "- \\mathbf X^+ (\\partial \\mathbf X) \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top (\\partial \\mathbf X)^\\top (1-\\mathbf X \\mathbf X^+)\n",
    "+ (1-\\mathbf X^+ \\mathbf X)(\\partial \\mathbf X)^\\top \\mathbf X{^+}^\\top \\mathbf X^+\n",
    "\\end{aligned}\\label{dpinv}\\end{equation}\n",
    "\n",
    "Since $\\mathbf X$ is $N\\times K$ with rank $K$, $\\mathbf X^+ \\mathbf X$ is full-rank. Therefore $\\mathbf X^+\\mathbf X = \\mathbf I_k$ and the final term  in $\\eqref{pinv}$ vanishes. The derivative of the pseudoinverse can now be written as: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial \\mathbf X^+\n",
    "&=\n",
    "- \\mathbf X^+ (\\partial \\mathbf X) \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top (\\partial \\mathbf X)^\\top ( \\mathbf I_n - \\mathbf X\\mathbf X^+ )\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-fiber",
   "metadata": {},
   "source": [
    "Since the derivative of a trace of a matrix-valued function is just the (transpose) of the scalar derivative, \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]\n",
    "&=\n",
    "\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left\\{\n",
    "- \\mathbf X^+ \\mathbf M \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top \\mathbf M^\\top ( \\mathbf I_n - \\mathbf X\\mathbf X^+ )\n",
    "\\right\\}^\\top\n",
    "\\\\\n",
    "&=\n",
    "-{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top + (\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top.\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-mention",
   "metadata": {},
   "source": [
    "Overall, we obtain the following Hessian-vector product: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\boldsymbol\\Lambda\\mathbf M^\\top\n",
    "+\n",
    "{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top \n",
    "- \n",
    "(\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top\n",
    "\\end{aligned}\\label{hvxx}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-regression",
   "metadata": {},
   "source": [
    "### ${\\boldsymbol\\Sigma}{=}{\\mathbf X}{{\\mathbf X}^{\\top}}$ when $\\mathbf X$ is full-rank\n",
    "\n",
    "Equations $\\eqref{jxx}$ and $\\eqref{hvxx}$ are also valid if $\\mathbf X$ is a rank-$L$ triangular (Choleskey) factorization of ${\\boldsymbol\\Sigma}$. In this case the pseudoinverse can be replaced by the full inverse, and various terms simplify: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D\n",
    "&=\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "-\n",
    "\\mathbf X^{-\\top}\n",
    "\\\\\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\mathbf x} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\boldsymbol\\Lambda\\mathbf M^\\top\n",
    "+\n",
    "\\mathbf X^{-\\top} \\mathbf M^\\top \\mathbf X^{-\\top}\n",
    "\\end{aligned}\n",
    "\\label{dxx}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-nerve",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}=\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$\n",
    "\n",
    "Let ${\\boldsymbol\\Sigma}=\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$, where $\\mathbf A$ is fixed and $\\mathbf v\\in\\mathbb R^L$ are free parameters. Define $\\operatorname{diag}[\\cdot]$ as an operator that constructs a diagonal matrix from a vector, or extracts the main diagonal from a matirx if its argument is a matrix. The gradient of $\\mathcal D$ in $\\mathbf v$ is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-subcommittee",
   "metadata": {},
   "source": [
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D &=\n",
    "\\partial_{\\mathbf X}\n",
    "\\tfrac 1 2 \\left\\{\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A\n",
    "\\right]\n",
    "-\n",
    "\\ln|\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A|\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac12\n",
    "\\left\\{\n",
    "\\operatorname{diag}[\\mathbf A \\boldsymbol\\Lambda \\mathbf A^\\top]\n",
    "- \\tfrac 1 {\\mathbf v}\n",
    "\\right\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-proposal",
   "metadata": {},
   "source": [
    "The hessian in $\\mathbf v$ is a matrix in this case, and is simply\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\operatorname{H}_{\\mathbf v}\n",
    "\\mathcal L &=\n",
    "\\tfrac 1 2 \\operatorname{diag}\\left[\\tfrac 1 {\\mathbf v^2}\\right].\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "This parameterization is useful for spatiotemporal inference problems, where the matrix $\\mathbf A$ represents a fixed convolution which can be evaluated using the Fast Fourier Transform (FFT). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-picking",
   "metadata": {},
   "source": [
    "## Inverse-diagonal approximation \n",
    "\n",
    "Let $\\boldsymbol\\Sigma^{-1} = \\boldsymbol\\Lambda + \\operatorname{diag}\\left[\\mathbf p\\right]$. To obtain the gradient in $\\mathbf p$, combine the derivatives $\\partial_{\\boldsymbol\\Sigma}\\mathcal D$ (Eq. $\\eqref{js}$) and $\\partial_{\\mathbf p}\\boldsymbol\\Sigma$ using the chain rule. If $\\mathcal f(\\boldsymbol\\Sigma)$ is a function of $\\boldsymbol\\Sigma$, and $\\boldsymbol\\Sigma(\\theta_i)$ is a function of a parameter $\\theta_i$, then the chain rule is (The Matrix Cookbook; Eq. 136):\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "{\\partial_{\\theta_i}}\\mathcal f = \n",
    "\\left<\n",
    "{\\partial_{\\boldsymbol\\Sigma}}\\mathcal f\n",
    ",\n",
    "{\\partial_{\\theta_i}}\\boldsymbol\\Sigma\n",
    "\\right>\n",
    "=\n",
    "\\sum_{kl}\n",
    "({\\partial_{\\boldsymbol\\Sigma_{kl}}}\\mathcal f)\n",
    "({\\partial_{\\theta_i}}\\boldsymbol\\Sigma_{kl})\n",
    "\\end{aligned}\\label{chain}\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-intent",
   "metadata": {},
   "source": [
    "From $\\eqref{js}$ we have $\\partial_{\\boldsymbol\\Sigma}\\mathcal D=\\tfrac 1 2 \\left(\\boldsymbol\\Lambda-\\boldsymbol\\Sigma^{-1}\\right)$; Since $\\boldsymbol\\Sigma^{-1} = \\boldsymbol\\Lambda + \\operatorname{diag}\\left[\\mathbf p\\right]$, this simplifies to: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Lambda - \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\right)\n",
    "\\\\\n",
    "&=\n",
    "-\\tfrac 1 2 \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\end{aligned}\\label{jsp}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-private",
   "metadata": {},
   "source": [
    "We also need $\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma$. Let $\\mathbf Y=\\boldsymbol\\Sigma^{-1}$. The derivative $\\partial\\mathbf Y^{-1}$ is given as identity (59) in The Matrix Cookbook as $\\partial\\mathbf Y^{-1} = -\n",
    "\\mathbf Y^{-1}(\\partial\\mathbf Y)\\mathbf Y^{-1}$. Using this, we can obtain $\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma\n",
    "&=\n",
    "\\partial_{\\mathbf p_i}\\mathbf Y^{-1}\n",
    "=\n",
    "-\\mathbf Y^{-1}\\left(\\partial_{\\mathbf p_i} \\mathbf Y\\right)\\mathbf Y^{-1}\n",
    "=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\left(\n",
    "\\partial_{\\mathbf p_i} \\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "\\boldsymbol\\Sigma\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\partial_{\\mathbf p_i} \\left[\\boldsymbol\\Lambda + \\operatorname{diag}[\\mathbf p_i] \\right]\n",
    "\\boldsymbol\\Sigma\n",
    "=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\mathbf J_{ii}\n",
    "\\boldsymbol\\Sigma\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\sigma_i\n",
    "\\boldsymbol\\sigma_i^\\top\n",
    "\\end{aligned}\\label{jsp2}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-huntington",
   "metadata": {},
   "source": [
    "where $\\boldsymbol\\sigma_i$ is the $i^{\\text{th}}$ row of $\\boldsymbol\\Sigma$ and $\\mathbf J_{ii}$ is a matrix which is zero evewhere, except for at index $(i,i)$, where it is $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-river",
   "metadata": {},
   "source": [
    "Applying $\\eqref{chain}$ to $\\eqref{jsp}$ and $\\eqref{jsp2}$ for a partcular element $\\mathbf p_i$ gives: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf p_i}\\mathcal D\n",
    "&=\n",
    "\\sum_{kl}\n",
    "[\\partial_{\\boldsymbol\\Sigma_{kl}} \\mathcal D]\n",
    "[\\partial_{\\mathbf p_i} \\boldsymbol\\Sigma_{kl}\n",
    "]\n",
    "=\n",
    "\\sum_{kl}\n",
    "\\left\\{\n",
    "-\\tfrac 1 2 \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\right\\}_{kl}\n",
    "\\left\\{\n",
    "-\n",
    "\\boldsymbol\\sigma_i\n",
    "\\boldsymbol\\sigma_i^\\top\n",
    "\\right\\}_{kl}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{kl}\n",
    "\\delta_{k=l} \\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "\\boldsymbol\\sigma_{il}\n",
    "=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{k}\n",
    "\\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{k}\n",
    "\\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}^2\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\mathbf p\n",
    "\\boldsymbol\\sigma_i^{\\circ 2}\n",
    "\\end{aligned}\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-karma",
   "metadata": {},
   "source": [
    "In matrix notation, this is:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf p}\\mathcal D\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\mathbf p\n",
    "\\boldsymbol\\Sigma^{\\circ 2}\n",
    "=\n",
    "\\tfrac 1 2 \n",
    "\\operatorname{diag}\\left[\n",
    "\\boldsymbol\\Sigma\n",
    "\\operatorname{diag}\\left[\n",
    "\\mathbf p\n",
    "\\right]\n",
    "\\boldsymbol\\Sigma\n",
    "\\right],\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "where $(\\cdot)^{\\circ 2}$ denotes the element-wise square of a vector or matrix. \n",
    "\n",
    "The Hessian-vector product is cumbersome, since each term in the expression $\\boldsymbol\\Sigma\\left(\\operatorname{diag}\\left[\\mathbf p\\right]\\right)\n",
    "\\boldsymbol\\Sigma$ depends on $\\mathbf p$. In the case of the log-linear Poisson GLM, the gradient $\\eqref{jp}$ simplifies further and optimization becomes tractable. We will explore this further in later notes. \n",
    "\n",
    "This parameterization resembles the closed-form covariance update for a linear, Gaussian model, where $1/\\mathbf p$ is a vector of measurement noise variances. It is also a useful parameterization for variational Bayesian solutions for non-conjugate Generalized Linear Models (GLMs), where $\\mathbf p$ becomes a free parameter to be estimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-mount",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}=\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$\n",
    "\n",
    "Let ${\\boldsymbol\\Sigma}=\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$, where $\\mathbf Q\\in\\mathbb R^{K{\\times}K}$; $K{<}L$  is the free parameter and $\\mathbf F\\in\\mathbb R^{K{\\times}L}$ is a fixed transformation. If $\\mathbf Q$ is a lower-triangular matrix, then this approximation involves optimizing $K(K+1)/2$ parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-forestry",
   "metadata": {},
   "source": [
    "Since the trace is invariant under cyclic permutation,\n",
    "$\\operatorname{tr}\\left[\\boldsymbol\\Lambda\\mathbf F^\\top\\mathbf Q\\mathbf Q^\\top\\mathbf F\\right]=\\operatorname{tr}\\left[\\mathbf F  \\boldsymbol\\Lambda\\mathbf F^\\top \\mathbf Q\\mathbf Q^\\top\\right]$. The derivatives have the same form as $\\eqref{dxx}$ with $\\tilde{\\boldsymbol\\Lambda} = \\mathbf F\\boldsymbol\\Lambda\\mathbf F^\\top$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\partial_{\\mathbf Q}\n",
    "\\mathcal D\n",
    "&=\n",
    "\\tilde{\\boldsymbol\\Lambda}\n",
    "\\mathbf Q \n",
    "-\n",
    "\\mathbf Q^{-\\top}\n",
    "\\\\\n",
    "&=\n",
    "\\mathbf F\\boldsymbol\\Lambda\\mathbf F^\\top\n",
    "\\mathbf Q \n",
    "-\n",
    "\\mathbf Q^{-\\top}\n",
    "\\\\\n",
    "\\partial_{\\mathbf Q} \\left< \\partial_{\\mathbf Q} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\tilde{\\boldsymbol\\Lambda}\\mathbf M^\\top\n",
    "+\n",
    "\\mathbf Q^{-\\top} \\mathbf M^\\top \\mathbf Q^{-\\top}\n",
    "\\\\\n",
    "&=\n",
    "\\mathbf F\\boldsymbol\\Lambda\\mathbf F^\\top\\mathbf M^\\top\n",
    "+\n",
    "\\mathbf Q^{-\\top} \\mathbf M^\\top \\mathbf Q^{-\\top}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-malaysia",
   "metadata": {},
   "source": [
    "This form is convenient for spatiotemporal inference problems that are sparse in frequency space. In this application, $\\mathbf F$ corresponds a (unitary) Fourier transform with all by $K$ of the resulting frequency components discarded. The product of $\\mathbf F$ with a vector $\\mathbf v$  can be computed in $\\mathcal O[L \\log(L)]$ time using the Fast Fourier Transform (FFT). Alternatively, if $K\\le \\mathcal O(\\log(L))$, it is faster to simply multiply $\\mathbf F\\mathbf v$ directly. Furthermore, if $\\mathbf F$ is semi-orthogonal ($\\mathbf F\\mathbf F^\\top = \\mathbf I$), then calculation of $\\mathbf F^\\top \\mathbf Q$ can be re-used (for example $\\operatorname{diag}[\\boldsymbol\\Sigma] = [(\\mathbf F^\\top \\mathbf Q)^{\\circ 2}]^\\top \\mathbf 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-jersey",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These notes provide the gradients and Hessian-vector products for four simplified parameterizations of the posterior covariance matrix for variational Gaussian process inference. These expressions can be used with Krylov-subspace solvers to compute the Newton-Raphson update to optimize $\\mathbf \\Sigma$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-berlin",
   "metadata": {},
   "source": [
    "We evaluated the following parameterizations for $\\boldsymbol\\Sigma$: \n",
    "1. $\\boldsymbol\\Sigma$:\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "\\\\\n",
    "\\partial\n",
    "\\left< \\partial,\\mathbf M\\right>\n",
    "&=\n",
    "\\frac 1 2 \n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\mathbf M^\\top\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\end{aligned}\\label{hvs}\\end{equation}\n",
    "\n",
    "2. $\\boldsymbol\\Sigma\\approx\\mathbf X\\mathbf X^\\top$:\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial\n",
    "&=\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "-\n",
    "{\\mathbf X^{+}}^\\top.\n",
    "\\\\\n",
    "\\partial\\left< \\partial, \\mathbf M \\right>\n",
    "&=\n",
    "\\boldsymbol\\Lambda\\mathbf M^\\top\n",
    "+\n",
    "{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top \n",
    "- \n",
    "(\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top\n",
    "\\end{aligned}\\label{hvxx}\\end{equation}\n",
    "\n",
    "3. $\\boldsymbol\\Sigma\\approx\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$:\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial\n",
    "&=\n",
    "\\tfrac12\n",
    "\\left\\{\n",
    "\\operatorname{diag}[\\mathbf A \\boldsymbol\\Lambda \\mathbf A^\\top]\n",
    "- \\tfrac 1 {\\mathbf v}\n",
    "\\right\\}\n",
    "\\\\\n",
    "\\partial\\left< \\partial, \\mathbf u \\right>\n",
    "&=\n",
    "\\tfrac 1 2 \\left[\\tfrac 1 {\\mathbf v^2}\\right]^\\top \\mathbf u\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "4. $\\boldsymbol\\Sigma\\approx[\\boldsymbol\\Lambda + \\operatorname{diag}[\\mathbf p]]^{-1}$:\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\mathbf p\n",
    "\\boldsymbol\\Sigma^{\\circ 2}\n",
    "=\n",
    "\\tfrac 1 2 \n",
    "\\operatorname{diag}\\left[\n",
    "\\boldsymbol\\Sigma\n",
    "\\operatorname{diag}\\left[\n",
    "\\mathbf p\n",
    "\\right]\n",
    "\\boldsymbol\\Sigma\n",
    "\\right],\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "\n",
    "5. $\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\partial\n",
    "&=\n",
    "\\mathbf F\\boldsymbol\\Lambda\\mathbf F^\\top\n",
    "\\mathbf Q \n",
    "-\n",
    "\\mathbf Q^{-\\top}\n",
    "\\\\\n",
    "\\partial\\left< \\partial, \\mathbf M \\right>\n",
    "&=\n",
    "\\mathbf F\\boldsymbol\\Lambda\\mathbf F^\\top\\mathbf M^\\top\n",
    "+\n",
    "\\mathbf Q^{-\\top} \\mathbf M^\\top \\mathbf Q^{-\\top}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-point",
   "metadata": {},
   "source": [
    "In future notes, we will consider the full derivatives requires for variational latent Gaussian-process inference for the Poisson and probit generalized linear models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
