{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-dispatch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n",
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],a\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],a\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-scientist",
   "metadata": {},
   "source": [
    "# Derivatives of KL-Divergence for various parameterizations of a multivariate Gaussian covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-nursing",
   "metadata": {},
   "source": [
    "> *These notes provide the derivatives of the KL-divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]$ between two multivariate Gaussian distributions $Q(\\mathbf z)$ and $\\Pr(\\mathbf z)$ with respect to various parameterizations of the covariance matrix of $Q$. This is useful for variational Gaussian process inference, where clever parameterizations of the posterior covariance are required to make the problem computationally tractable. Tables for differentiating matrix-valued functions can be found in [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-correlation",
   "metadata": {},
   "source": [
    "Consider a Bayesian inference problem with observed data $\\mathbf y$, and latent variables $\\mathbf z$ to be inferred. Assume that the prior $\\Pr(\\mathbf z)$ is a multivariate Gaussian, $\\mathbf z\\sim\\mathcal N(\\boldsymbol\\mu_0,\\boldsymbol\\Sigma_0)$. We are interested in the posterior distribution of $\\mathbf z$ given the observations $\\mathbf y$ \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\Pr(\\mathbf z | \\mathbf y) = \\Pr(\\mathbf y | \\mathbf z)\n",
    "\\frac{\\Pr(\\mathbf z)}{\\Pr(\\mathbf y)}\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "Unless $\\Pr(\\mathbf y | \\mathbf z)$ is also a multivariate Gaussian, there will be no simple closed-form for the posterior $\\Pr(\\mathbf z | \\mathbf y)$. The variational Bayesian approach approximates this posterior $\\Pr(\\mathbf z | \\mathbf y)$ with a simpler more tractable distribution. Here, we consider the case of multivariate Gaussian distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-indonesian",
   "metadata": {},
   "source": [
    "\\begin{equation}\\begin{aligned}\n",
    "\\Pr(\\mathbf z | \\mathbf y)\n",
    "\\approx Q(\\mathbf z) = \\mathcal N(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q)\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-hayes",
   "metadata": {},
   "source": [
    "The parameters $\\Theta = (\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q)$ of $Q(\\mathbf z)$ are fit buy minimizing the [Kullback-Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) from the true posterior $\\Pr(\\mathbf z | \\mathbf y)$ to $Q(\\mathbf z)$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\Theta\\gets\\underset{\\Theta}{\\operatorname{argmin}}\n",
    "D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z | \\mathbf y)\\right]\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-married",
   "metadata": {},
   "source": [
    "This is equivalent to minimizing the KL divergenece $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]$ from the prior to the posterior, while maximizing the expected log-likelihood $\\left<\\Pr(\\mathbf y | \\mathbf z)\\right>_Q$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q\n",
    "&\\gets\n",
    "\\underset{\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q}{\\operatorname{argmin}}\n",
    "\\mathcal L(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q)\n",
    "\\\\\n",
    "\\mathcal L(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q)\n",
    "&=\n",
    "D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z | \\mathbf y)\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left<\n",
    "\\ln\\frac{Q(\\mathbf z)}\n",
    "{\\Pr(\\mathbf z | \\mathbf y)}\n",
    "\\right>_{Q}\n",
    "\\\\\n",
    "&=\n",
    "\\left<\n",
    "\\ln Q(\\mathbf z)\n",
    "\\right>_Q\n",
    "-\n",
    "\\left<\n",
    "\\ln\\Pr(\\mathbf z | \\mathbf y)\n",
    "\\right>_Q\n",
    "\\\\\n",
    "&=\n",
    "\\left<\n",
    "\\ln Q(\\mathbf z)\n",
    "\\right>_Q\n",
    "-\n",
    "\\left<\n",
    "\\ln\\left(\n",
    "\\Pr(\\mathbf y | \\mathbf z)\n",
    "\\frac\n",
    "{\\Pr(\\mathbf z)}\n",
    "{\\Pr(\\mathbf y)}\n",
    "\\right)\n",
    "\\right>_Q\n",
    "\\\\\n",
    "&=\n",
    "\\left<\\ln \\frac{Q(\\mathbf z)}{\\Pr(\\mathbf z)}\\right>_Q-\\left<\\ln\\Pr(\\mathbf y | \\mathbf z)\n",
    "\\right>_Q\n",
    "+\\left<\\ln\\Pr(\\mathbf y)\\right>_Q\\\\\n",
    "&=\n",
    "D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]\n",
    "-\\left<\\ln\\Pr(\\mathbf y | \\mathbf z)\\right>_Q+c,\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-chosen",
   "metadata": {},
   "source": [
    "where $\\langle\\cdot\\rangle_Q$ denotes averaging with respect to $Q(\\mathbf z)$, and the probability of the observations $\\Pr(\\mathbf y)$ can be neglected, since it is a constant (\"$c$\") that does not depend on $Q$. Since both $Q(\\mathbf z)$ and $\\Pr(\\mathbf z)$ are multivariate Gaussian, the KL divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]$ [has the closed form](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]\n",
    "&=\n",
    "\\frac 1 2 \\left\\{\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)^\\top \n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)\n",
    "+\n",
    "\\operatorname{tr}\\left(\n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "\\boldsymbol\\Sigma_q\n",
    "\\right)\n",
    "+\n",
    "\\ln|\n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "\\boldsymbol\\Sigma_q\n",
    "|\n",
    "\\right\\}\n",
    "+\\text{constant.}\n",
    "\\end{aligned}\n",
    "\\label{dkl}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-boulder",
   "metadata": {},
   "source": [
    "In these notes, we consider the first and second derivatives of $\\mathcal L$ with respect to the posterior parameters $(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma_q)$. Gradients of the Gaussian LK-divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]$ are solved explicitly. Gradients of the expected negative log-likelihood, $\\mathcal \\ell = -\\left<\\ln\\Pr(\\mathbf y)\\right>_Q$, depend on the specific choice for the likelihood and will be addressed elsewheres. The derivatives of $\\mathcal L$ in $\\boldsymbol\\mu_q$ are straightforward:\n",
    "\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\mu_q}\\mathcal L\n",
    "&=\n",
    "\\partial_{\\boldsymbol\\mu_q}\\ell\n",
    "+\n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "(\\boldsymbol\\mu_q-\\boldsymbol\\mu_z)\n",
    "\\\\\n",
    "\\operatorname H_{\\boldsymbol\\mu_q}\\mathcal L\n",
    "&=\n",
    "\\operatorname H_{\\boldsymbol\\mu_q}\\ell\n",
    "+\n",
    "\\boldsymbol\\Sigma_0^{-1}.\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "The remainder of the document considers derivatives in $\\boldsymbol\\Sigma_q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-kansas",
   "metadata": {},
   "source": [
    "# Derivatives for various parameterizations of ${\\boldsymbol\\Sigma}_q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-absolute",
   "metadata": {},
   "source": [
    "We evaluate the following approximations for the posterior covariance: \n",
    "1. Optimizing the full $\\boldsymbol\\Sigma_q$ directly. This is generally inadvisable, but is provided for completeness and comparison to the other approaches. \n",
    "3. A low-rank approximation $\\boldsymbol\\Sigma_q\\approx\\mathbf X\\mathbf X^\\top$, where $\\mathbf X$ is a tall, thin matrix. This captures the principle subspace of $\\boldsymbol\\Sigma_q$.\n",
    "1. A diagonal approximation $\\boldsymbol\\Sigma_q\\approx\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$, where $\\mathbf A$ is a fixed spatial convolution.\n",
    "2. A modification of (3) where $\\mathbf A$ is a local Gaussian blur.\n",
    "2. An inverse diagonal approximation $\\boldsymbol\\Sigma_q\\approx[\\boldsymbol\\Sigma_0^{-1} + \\operatorname{diag}[\\mathbf p]]^{-1}$, where $\\boldsymbol\\Sigma_0$ is the prior covariance and $\\mathbf p$ is a vector of the inverse variance (precision) at each spatial location.\n",
    "4. A reduced Fourier-space representation $\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$, where $\\mathbf F$ is the unitary 2D Fourier transform, with frequencies that are zero in the prior $\\boldsymbol\\Sigma_0$ discarded, and $\\mathbf Q$ is a lower-triangular matrix. Since the posterior cannot assign probability mass where the prior $\\boldsymbol\\Sigma_0$ is zero, this fully parameterizes $\\boldsymbol\\Sigma_q$ in a compact way. The Fourier transform can be computed quickly using the Fast Fourier Transform (FFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-facility",
   "metadata": {},
   "source": [
    "## Fully-parameterizing $\\boldsymbol\\Sigma_q$\n",
    "\n",
    "It is unusual to optimize the covarince of the variational posterior, $\\boldsymbol\\Sigma_q$, directly. For one, this matrix must be constrained to be positive definite, and it is easier to ensure this via e.g. parameterising in terms of a triangular factor $\\mathbf X$ such that $\\boldsymbol\\Sigma_q = \\mathbf X\\mathbf X^\\top$. Additionally, in most Gaussian-process inference problems, $\\boldsymbol\\Sigma_q$ will be sufficiently large that optimizing the full-rank representation is computatoinally prohibative. \n",
    "\n",
    "Neverheless, we present the derivatives in $\\boldsymbol\\Sigma_q$ here. It will be useful to compare the derivatives for various parameterizations of $\\boldsymbol\\Sigma_q$ to these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-roman",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "The gradient and Hessian of $\\mathcal L$ with respect to $\\boldsymbol\\Sigma_q$ are more complicated. An analytic derivative in $\\boldsymbol\\Sigma_q$ can be obtained using the various identities provided in [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html).\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\\mathcal L\n",
    "=\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\\ell\n",
    "+\n",
    "\\frac 1 2 \\left(\n",
    "\\boldsymbol\\Sigma_z^{-1}\n",
    "+\n",
    "\\boldsymbol\\Sigma_q^{-\\top}\n",
    "\\right)\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-machinery",
   "metadata": {},
   "source": [
    "### Hessian-vector-product \n",
    "\n",
    "The Hessian in $\\boldsymbol\\Sigma_q$ is a fourth-order tensor. It's simpler to express the Hessian in terms of a Hessian-vector product, which can be used with [Krylov subspace](https://en.wikipedia.org/wiki/Krylov_subspace) solvers to efficiently compute the update in Newton's method. These solvers are implemented, for example, in Matlab and Scipy's \"minres\" function. Considering an $N\\times N$ matrix $\\mathbf M$, the Hessian-vector product is given by \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\n",
    "\\left< \\partial_{\\boldsymbol\\Sigma_q}\\mathcal L,\\mathbf M\\right>\n",
    "=\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\n",
    "\\operatorname{tr}\\left[\n",
    "(\\partial_{\\boldsymbol\\Sigma_q}\\mathcal L)^\\top \\mathbf M\\right]\n",
    "=\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\\left< \\partial_{\\boldsymbol\\Sigma_q}\\ell,\\mathbf M\\right>\n",
    "+\n",
    "\\operatorname{tr}\\left[\\frac 1 2 \\left(\\boldsymbol\\Sigma_0^{-1}+\\boldsymbol\\Sigma_q^{-\\top}\\right)^\\top\\mathbf M\\right]\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-gravity",
   "metadata": {},
   "source": [
    "where $\\langle\\cdot,\\cdot\\rangle$ denotes the scalar product. The Hessian-vector product for terms arising from the KL divergence is straightforward:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\n",
    "\\operatorname{tr}\\left[\n",
    "\\frac 1 2 \\left(\n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "+\n",
    "\\boldsymbol\\Sigma_q^{-\\top}\n",
    "\\right)\n",
    "^\\top\n",
    "\\mathbf M\n",
    "\\right]\n",
    "&=\n",
    "\\frac 1 2 \n",
    "\\partial_{\\boldsymbol\\Sigma_q}\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Sigma_q^{-1}\n",
    "\\mathbf M\n",
    "\\right]\n",
    "=\n",
    "-\\frac 1 2 \n",
    "\\boldsymbol\\Sigma_q^{-1}\n",
    "\\mathbf M^\\top\n",
    "\\boldsymbol\\Sigma_q^{-1}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-clark",
   "metadata": {},
   "source": [
    "### ${\\boldsymbol\\Sigma}_q\\approx{\\mathbf X}{{\\mathbf X}^{\\top}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-villa",
   "metadata": {},
   "source": [
    "We consider an approximate posterior covariance of the form\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\boldsymbol\\Sigma^{-1} &\\approx \\mathbf X \\mathbf X^\\top,\\,\\,\\,\\,\\mathbf X\\in\\mathbb R^{L^2\\times K}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-tennessee",
   "metadata": {},
   "source": [
    "where $\\mathbf X$ is a tall, thin matrix, with as many rows as there are spatial bins ($L^2$), and $K<L^2$ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-knowing",
   "metadata": {},
   "source": [
    "If $\\mathbf X$ is not full rank, the log-determinant $\\ln|{\\boldsymbol\\Sigma}_q|=\\ln|\\mathbf X\\mathbf X^\\top|$ in $\\eqref{dkl}$ will diverge to $-\\infty$, due to the zero eigenvalues in the null space of $\\mathbf X$. However, since this null-space is not being optimized, it does not affect our gradient. It is sufficient to replace the log-determinant with that of the reduced-rank representation, $\\ln|\\mathbf X^\\top\\mathbf X|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-death",
   "metadata": {},
   "source": [
    "For the graident, the Matrix Cookbook provides $\\partial_{\\mathbf X}\\ln|\\mathbf X^\\top\\mathbf X| = {\\mathbf X^{+}}^\\top$\n",
    "\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal L &=\n",
    "(\\partial_{\\boldsymbol\\Sigma_q}\\ell) \\mathbf X\n",
    "+\n",
    "\\boldsymbol\\Sigma_0^{-1}\n",
    "\\mathbf X - {\\mathbf X^{+}}^\\top\n",
    "\\end{aligned}\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-magnet",
   "metadata": {},
   "source": [
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma_q} \\mathcal L , \\mathbf M \\right>\n",
    "&=\n",
    "\\partial_{\\mathbf X} \\left< \n",
    "\\left(\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\\ell\n",
    "+ \\boldsymbol\\Sigma_0^{-1}\\right)\\mathbf X - {\\mathbf X^{+}}^\\top\n",
    ", \\mathbf M \\right>\n",
    "\\\\\n",
    "&=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\mathbf X^\\top\n",
    "\\left(\n",
    "\\partial_{\\boldsymbol\\Sigma_q}\\ell\n",
    "\\right)\n",
    "\\mathbf M\n",
    "+ \n",
    "\\mathbf X^\\top \\boldsymbol\\Sigma_0^{-1}\\mathbf M\n",
    "- \n",
    "{\\mathbf X^{+}}\\mathbf M\n",
    "\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left(\\partial_{\\boldsymbol\\Sigma_q}\\ell\\right)\n",
    "\\mathbf M\n",
    "+ \n",
    "\\boldsymbol\\Sigma_0^{-1}\\mathbf M\n",
    "+{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top - (\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-training",
   "metadata": {},
   "source": [
    " (These same derivaties also work to optimize the full rank ${\\boldsymbol\\Sigma}_q$ if $\\mathbf X$ is a triangular (Choleskey) factorization of ${\\boldsymbol\\Sigma}_q$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-technique",
   "metadata": {},
   "source": [
    "The gradient and Hessian of this loss function in $\\mathbf v$ are : \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "%%%% GRADIENT IN v\n",
    "\\operatorname{\\nabla}_{\\mathbf v}\n",
    "\\mathcal L &=\\tfrac 1 2 \\left\\{\n",
    "\\mathbf G^\\top (\\mathbf n \\circ \\langle\\boldsymbol\\lambda\\rangle)\n",
    "+ \\operatorname{diag}[\\mathbf A \\boldsymbol\\Sigma_0^{-1} \\mathbf A^\\top]\n",
    "- \\tfrac 1 {\\mathbf v}\n",
    "\\right\\}\n",
    "\\\\\n",
    "%%%% HESSIAN IN v\n",
    "\\operatorname{H}_{\\mathbf v}\n",
    "\\mathcal L &=\\tfrac 1 2 \\left\\{\n",
    "\\tfrac 1 2 \n",
    "\\mathbf G \\operatorname{diag}[\\mathbf n \\circ \\langle\\boldsymbol\\lambda\\rangle] \\, \\mathbf G \n",
    "+ \\operatorname{diag}\\left[\\tfrac 1 {\\mathbf v^2}\\right]\n",
    "\\right\\}\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "This follows from the usual matrix and scalar derivatives, with the exception of the term $\\mathbf n^\\top \\langle\\boldsymbol\\lambda\\rangle$. These can be obtained by considering the derivative with respect to single elements of $\\mathbf v$ (note: $\\mathbf G$ is symmetric): \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\tfrac{d}{dv_j} \\mathbf n^\\top\\langle\\boldsymbol\\lambda\\rangle \n",
    "&=  \\mathbf n^\\top\n",
    "\\tfrac{d}{dv_j}  \\langle\\boldsymbol\\lambda\\rangle \n",
    "\\\\\n",
    "&=  \\mathbf n^\\top\n",
    "\\tfrac{d}{dv_j} \\exp\\left(\\boldsymbol\\mu + \\tfrac12\\mathbf G\\mathbf v\\right)\n",
    "\\\\\n",
    "&= \\tfrac{d}{dv_j} \\sum_k\n",
    "n_k \\exp\\left[\\mu_k + \\tfrac12(\\mathbf G\\mathbf v)_k\\right]\n",
    "\\\\\n",
    "&= \\sum_k\n",
    "n_k \\left[\n",
    "\\langle\\lambda_k\\rangle\n",
    "\\cdot \\tfrac12 \\tfrac{d}{dv_j} (\\mathbf G\\mathbf v)_k\n",
    "\\right]\n",
    "\\\\\n",
    "&= \n",
    "\\tfrac12 \\sum_k\n",
    " n_k \n",
    "\\langle\\boldsymbol\\lambda_k\\rangle\n",
    "\\textstyle \\mathbf G_{kj} \n",
    "\\\\\n",
    "&= \n",
    "\\tfrac12 \\left[ \\mathbf G^\\top(\\mathbf n\\circ\\langle\\boldsymbol\\lambda\\rangle) \\right]_j\n",
    "\\\\\\\\\n",
    "\\tfrac{d^2}{dv_iv_j} \\mathbf n^\\top\\langle\\boldsymbol\\lambda\\rangle \n",
    "&= \n",
    "\\tfrac12 \\left[ \\sum_k n_k \\tfrac{d}{dv_i} \\langle\\boldsymbol\\lambda_k\\rangle\\textstyle \\mathbf G_{kj} \\right]_j\n",
    "\\\\\n",
    "&= \n",
    "\\tfrac14 \\left[ \\sum_k n_k \n",
    "\\langle\\boldsymbol\\lambda_k\\rangle\n",
    "\\textstyle \\mathbf G_{jk} \n",
    "\\textstyle \\mathbf G_{ki} \n",
    "\\right]_j\n",
    "\\\\\n",
    "&= \n",
    "\\tfrac 1 4 \\left[\\mathbf G \\operatorname{diag}[\\mathbf n\\circ \\langle\\boldsymbol\\lambda\\rangle] \\, \n",
    "\\mathbf G\n",
    "\\right]_{ij}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
