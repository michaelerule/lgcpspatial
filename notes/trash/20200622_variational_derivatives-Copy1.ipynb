{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ultimate-significance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n",
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-ghost",
   "metadata": {},
   "source": [
    "# Derivatives of KL-Divergence for various parameterizations of a multivariate Gaussian covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-attribute",
   "metadata": {},
   "source": [
    "> *These notes provide the derivatives of the KL-divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| P(\\mathbf z)\\right]$ between two multivariate Gaussian distributions $Q(\\mathbf z)$ and $P(\\mathbf z)$ with respect to various parameterizations of the covariance matrix of $Q$. This is useful for variational Gaussian process inference, where clever parameterizations of the posterior covariance are required to make the problem computationally tractable. Tables for differentiating matrix-valued functions can be found in [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-video",
   "metadata": {},
   "source": [
    "Consider two multivariate Gaussian distributions $Q(\\mathbf z) = \\mathcal N(\\boldsymbol\\mu_q,\\boldsymbol\\Sigma(\\theta))$ and $P(\\mathbf z) = \\mathcal N(\\boldsymbol\\mu_0,\\boldsymbol\\Sigma_0 = \\boldsymbol\\Lambda^{-1})$ with dimension $L$. The KL divergence $D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| P(\\mathbf z)\\right]$ [has the closed form](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\mathcal D \n",
    ":= &\\, D_{\\text{KL}}\\left[ Q(\\mathbf z) \\| \\Pr(\\mathbf z)\\right]\n",
    "\\\\\n",
    "= &\\,\n",
    "\\tfrac 1 2 \\left\\{\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)^\\top \n",
    "\\boldsymbol\\Lambda\n",
    "(\\boldsymbol\\mu_0-\\boldsymbol\\mu_q)\n",
    "\\right.\n",
    "\\\\\n",
    "&\\,\\left.+\n",
    "\\operatorname{tr}\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\boldsymbol\\Sigma\n",
    "\\right)\n",
    "-\n",
    "\\ln|\\boldsymbol\\Sigma|\n",
    "-\n",
    "\\ln|\\boldsymbol\\Lambda|\n",
    "\\right\\}\n",
    "+\\text{constant.}\n",
    "\\end{aligned}\n",
    "\\label{dkl}\n",
    "\\end{equation}\n",
    "\n",
    "In variational Bayesian inference, we minimize $\\mathcal D$ while maximizing the expected log-probability of some observations with respect to $Q(\\mathbf z)$. Closed-form derivatives of $\\mathcal D$ in terms of the parameters of $Q$ are useful for manually optimizing code for larger problems. The derivatives of $\\mathcal D$ in terms of $\\boldsymbol\\mu_q$ are straightforward ($\\partial_{\\boldsymbol\\mu_q}\\mathcal D=\\boldsymbol\\Lambda(\\boldsymbol\\mu_q-\\boldsymbol\\mu_z)$\n",
    "and \n",
    "$\\operatorname H_{\\boldsymbol\\mu_q}\\mathcal D=\\boldsymbol\\Lambda$). In these notes, we explore derivatives of $\\mathcal D$ with respect to a few different parameterizations (\"$\\theta$\") of $\\boldsymbol\\Sigma(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-quebec",
   "metadata": {},
   "source": [
    "We evaluate the following parameterizations for $\\boldsymbol\\Sigma$: \n",
    "1. Optimizing the full $\\boldsymbol\\Sigma$ directly\n",
    "3. $\\boldsymbol\\Sigma\\approx\\mathbf X\\mathbf X^\\top$\n",
    "1. $\\boldsymbol\\Sigma\\approx\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$\n",
    "2. $\\boldsymbol\\Sigma\\approx[\\boldsymbol\\Lambda + \\operatorname{diag}[\\mathbf p]]^{-1}$\n",
    "4. $\\mathbf F^\\top \\mathbf Q \\mathbf Q^\\top \\mathbf F$, where $\\mathbf Q\\in\\mathbb R^{K{\\times}K}$, $K{<}L$ and $\\mathbf F\\in\\mathbb R^{K{\\times}L}$, \n",
    "$\\mathbf F\\mathbf F^\\top = \\mathbf I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-charles",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}$\n",
    "\n",
    "We first obtain gradients of $\\mathcal D$ in ${\\boldsymbol\\Sigma}$, assuming ${\\boldsymbol\\Sigma}$ is full-rank. These can be used to derive gradients in $\\theta$ for some parameterizations ${\\boldsymbol\\Sigma}(\\theta)$ using the chain rule. The gradient of $\\mathcal D$ in $\\boldsymbol\\Sigma$ can be obtained using identities (57) and (100) in [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html):\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "&=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\left\\{\n",
    "\\operatorname{tr}\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\boldsymbol\\Sigma\n",
    "\\right)\n",
    "-\n",
    "\\ln|\\boldsymbol\\Sigma|\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right).\n",
    "\\end{aligned}\\label{js}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-favor",
   "metadata": {},
   "source": [
    "The Hessian in $\\boldsymbol\\Sigma$ is a fourth-order tensor. It's simpler to express the Hessian in terms of a Hessian-vector product, which can be used with [Krylov subspace](https://en.wikipedia.org/wiki/Krylov_subspace) solvers to efficiently compute the update in Newton's method. Considering an $L{\\times}L$ matrix $\\mathbf M$, the Hessian-vector product is given by \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\left[\n",
    "\\mathbf H_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "\\right] \\mathbf M\n",
    "&=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\left< \\partial_{\\boldsymbol\\Sigma}\\mathcal D,\\mathbf M\\right>\n",
    "=\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "(\\partial_{\\boldsymbol\\Sigma}\\mathcal D)^\\top \\mathbf M\\right],\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-algebra",
   "metadata": {},
   "source": [
    "where $\\langle\\cdot,\\cdot\\rangle$ denotes the scalar (Frobenius) product. This is given by identity (124) in the Matrix Cookbook:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "\\frac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "^\\top\n",
    "\\mathbf M\n",
    "\\right]\n",
    "&=\n",
    "-\\frac 1 2 \n",
    "\\partial_{\\boldsymbol\\Sigma}\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\mathbf M\n",
    "\\right]\n",
    "=\n",
    "\\frac 1 2 \n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\mathbf M^\\top\n",
    "\\boldsymbol\\Sigma^{-1}.\n",
    "\\end{aligned}\\label{hvs}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-gravity",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}{\\approx}{\\mathbf X}{{\\mathbf X}^{\\top}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-command",
   "metadata": {},
   "source": [
    "We consider an approximate posterior covariance of the form\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\boldsymbol\\Sigma &\\approx \\mathbf X \\mathbf X^\\top,\\,\\,\\,\\,\\mathbf X\\in\\mathbb R^{L\\times K}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-defeat",
   "metadata": {},
   "source": [
    "where $\\mathbf X$ is a rank-$K<L$ matrix with $L$ rows and $K$ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-animal",
   "metadata": {},
   "source": [
    "Since $\\mathbf X$ is not full rank, the log-determinant $\\ln|{\\boldsymbol\\Sigma}|=\\ln|\\mathbf X\\mathbf X^\\top|$ in $\\eqref{dkl}$ diverges, due to the zero eigenvalues in the null space of $\\mathbf X$. However, since this null-space is not being optimized, it does not affect our gradient. It is sufficient to replace the log-determinant with that of the reduced-rank representation, $\\ln|\\mathbf X^\\top\\mathbf X|$. Identity (55) in The Matrix Cookbook provides the derivative of this, $\\partial_{\\mathbf X}\\ln|\\mathbf X^\\top\\mathbf X| = 2 {\\mathbf X^{+}}^\\top$, where $\\cdot^+$ denotes the pseudoinverse. Combined with identity (112), this gives the following gradient of $\\mathcal D(\\mathbf X)$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D &=\n",
    "\\partial_{\\mathbf X}\n",
    "\\tfrac 1 2 \\left\\{\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X^\\top\\mathbf X\n",
    "\\right]\n",
    "-\n",
    "\\ln|\\mathbf X^\\top\\mathbf X|\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac12\n",
    "\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "+\n",
    "\\mathbf X \n",
    "\\boldsymbol\\Lambda\n",
    "\\right)\n",
    "-\n",
    "{\\mathbf X^{+}}^\\top.\n",
    "\\end{aligned}\n",
    "\\label{jxx}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-hello",
   "metadata": {},
   "source": [
    "The Hessian-vector product requires the derivative of $\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]$:\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\left(\n",
    "\\frac12\n",
    "\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "+\n",
    "\\mathbf X \n",
    "\\boldsymbol\\Lambda\n",
    "\\right)\n",
    "-\n",
    "{\\mathbf X^{+}}^\\top\n",
    "\\right)\n",
    "^\\top\n",
    "\\mathbf M\n",
    "\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\tfrac12\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "\\mathbf M\n",
    "+\n",
    "\\tfrac12\n",
    "\\mathbf X \n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf M\n",
    "-\n",
    "{\\mathbf X^{+}}\n",
    "\\mathbf M\n",
    "\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "\\mathbf M\n",
    "\\right]\n",
    "-\n",
    "\\partial_{\\mathbf X}\n",
    "\\operatorname{tr}\\left[\n",
    "{\\mathbf X^{+}}\n",
    "\\mathbf M\n",
    "\\right].\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-brave",
   "metadata": {},
   "source": [
    "Goulob and Pereya (1972) Eq. 4.12 gives the derivative of a fixed-rank pseudoinverse: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial \\mathbf X^+ =\n",
    "- \\mathbf X^+ (\\partial \\mathbf X) \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top (\\partial \\mathbf X)^\\top (1-\\mathbf X \\mathbf X^+)\n",
    "+ (1-\\mathbf X^+ \\mathbf X)(\\partial \\mathbf X)^\\top \\mathbf X{^+}^\\top \\mathbf X^+\n",
    "\\end{aligned}\\label{dpinv}\\end{equation}\n",
    "\n",
    "Since $\\mathbf X$ is $N\\times K$ with rank $K$, $\\mathbf X^+ \\mathbf X$ is full-rank. Therefore $\\mathbf X^+\\mathbf X = \\mathbf I_k$ and the final term  in $\\eqref{pinv}$ vanishes. The derivative of the pseudoinverse can now be written as: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial \\mathbf X^+\n",
    "&=\n",
    "- \\mathbf X^+ (\\partial \\mathbf X) \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top (\\partial \\mathbf X)^\\top ( \\mathbf I_n - \\mathbf X\\mathbf X^+ )\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-reason",
   "metadata": {},
   "source": [
    "Since the derivative of a trace of a matrix-valued function is just the (transpose) of the scalar derivative, \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]\n",
    "&=\n",
    "\\partial_{\\mathbf X} \\operatorname{tr}\\left[{\\mathbf X^{+}}\\mathbf M\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left\\{\n",
    "- \\mathbf X^+ \\mathbf M \\mathbf X^+\n",
    "+ \\mathbf X^+ \\mathbf X{^+}^\\top \\mathbf M^\\top ( \\mathbf I_n - \\mathbf X\\mathbf X^+ )\n",
    "\\right\\}^\\top\n",
    "\\\\\n",
    "&=\n",
    "-{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top + (\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top.\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-capture",
   "metadata": {},
   "source": [
    "Overall, we obtain the following Hessian-vector product: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\boldsymbol\\Lambda\\mathbf M^\\top\n",
    "+\n",
    "{\\mathbf X^+}^\\top \\mathbf M^\\top {\\mathbf X^+}^\\top \n",
    "- \n",
    "(\\mathbf I - {\\mathbf X^+}^\\top \\mathbf X^\\top) \\mathbf M \\mathbf X^+ {\\mathbf X^+}^\\top\n",
    "\\end{aligned}\\label{hvxx}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-laser",
   "metadata": {},
   "source": [
    "### ${\\boldsymbol\\Sigma}{=}{\\mathbf X}{{\\mathbf X}^{\\top}}$ when $\\mathbf X$ is full-rank\n",
    "\n",
    "Equations $\\eqref{jxx}$ and $\\eqref{hvxx}$ are also valid if $\\mathbf X$ is a rank-$L$ triangular (Choleskey) factorization of ${\\boldsymbol\\Sigma}$. In this case the pseudoinverse can be replaced by the full inverse, and various terms simplify: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D\n",
    "&=\n",
    "\\tfrac12\n",
    "\\left(\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf X \n",
    "+\n",
    "\\mathbf X \n",
    "\\boldsymbol\\Lambda\n",
    "\\right)\n",
    "-\n",
    "\\mathbf X^{-\\top}\n",
    "\\\\\n",
    "\\partial_{\\mathbf X} \\left< \\partial_{\\boldsymbol\\Sigma} \\mathcal D, \\mathbf M \\right>\n",
    "&=\n",
    "\\boldsymbol\\Lambda\\mathbf M^\\top\n",
    "+\n",
    "\\mathbf X^{-\\top} \\mathbf M^\\top \\mathbf X^{-\\top}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-swedish",
   "metadata": {},
   "source": [
    "## ${\\boldsymbol\\Sigma}=\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$\n",
    "\n",
    "Let ${\\boldsymbol\\Sigma}=\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A$, where $\\mathbf A$ is fixed and $\\mathbf v\\in\\mathbb R^L$ are free parameters. Define $\\operatorname{diag}[\\cdot]$ as an operator that constructs a diagonal matrix from a vector, or extracts the main diagonal from a matirx if its argument is a matrix. The gradient of $\\mathcal D$ in $\\mathbf v$ is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-scanner",
   "metadata": {},
   "source": [
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf X}\n",
    "\\mathcal D &=\n",
    "\\partial_{\\mathbf X}\n",
    "\\tfrac 1 2 \\left\\{\n",
    "\\operatorname{tr}\\left[\n",
    "\\boldsymbol\\Lambda\n",
    "\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A\n",
    "\\right]\n",
    "-\n",
    "\\ln|\\mathbf A^\\top \\operatorname{diag}[\\mathbf v] \\mathbf A|\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac12\n",
    "\\left\\{\n",
    "\\operatorname{diag}[\\mathbf A \\boldsymbol\\Lambda \\mathbf A^\\top]\n",
    "- \\tfrac 1 {\\mathbf v}\n",
    "\\right\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-klein",
   "metadata": {},
   "source": [
    "The hessian in $\\mathbf v$ is a matrix in this case, and takes a simple form: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\operatorname{H}_{\\mathbf v}\n",
    "\\mathcal L &=\n",
    "\\tfrac 1 2 \\operatorname{diag}\\left[\\tfrac 1 {\\mathbf v^2}\\right]\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-wireless",
   "metadata": {},
   "source": [
    "## Inverse-diagonal approximation \n",
    "\n",
    "Let\n",
    "$\\boldsymbol\\Sigma^{-1} = \\boldsymbol\\Lambda + \\operatorname{diag}\\left[\\mathbf p\\right]$.\n",
    "This parameterization resembles the closed-form covariance update for a linear, Gaussian model, where $1/\\mathbf p$ is a vector of measurement noise variances. It is also a useful parameterization for non-conjugate Generalized Linear Models (GLMs), where $\\mathbf p$ becomes a free parameter to be estimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-prize",
   "metadata": {},
   "source": [
    "To obtain the gradient in $\\mathbf p$, combine the derivatives $\\partial_{\\boldsymbol\\Sigma}\\mathcal D$ (Eq. $\\eqref{js}$) and $\\partial_{\\mathbf p}\\boldsymbol\\Sigma$ using the chain rule. From $\\eqref{js}$ we have $\\partial_{\\boldsymbol\\Sigma}\\mathcal D=\\tfrac 1 2 \\left(\\boldsymbol\\Lambda-\\boldsymbol\\Sigma^{-1}\\right)$; Since $\\boldsymbol\\Sigma^{-1} = \\boldsymbol\\Lambda + \\operatorname{diag}\\left[\\mathbf p\\right]$, this simplifies to: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\boldsymbol\\Sigma}\\mathcal D\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \\left(\n",
    "\\boldsymbol\\Lambda\n",
    "-\n",
    "\\boldsymbol\\Lambda - \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\right)\n",
    "\\\\\n",
    "&=\n",
    "-\\tfrac 1 2 \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\end{aligned}\\label{}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-equipment",
   "metadata": {},
   "source": [
    "We also need $\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma$. Let $\\mathbf Y=\\boldsymbol\\Sigma^{-1}$. The derivative $\\partial\\mathbf Y^{-1}$ is given as identity (59) in The Matrix Cookbook as $\\partial\\mathbf Y^{-1} = -\n",
    "\\mathbf Y^{-1}(\\partial\\mathbf Y)\\mathbf Y^{-1}$. Using this, we can obtain $\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma$:\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf p_i}\\boldsymbol\\Sigma\n",
    "&=\n",
    "\\partial_{\\mathbf p_i}\\mathbf Y^{-1}\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\mathbf Y^{-1}\n",
    "\\left(\n",
    "\\partial_{\\mathbf p_i} \\mathbf Y\n",
    "\\right)\n",
    "\\mathbf Y^{-1}\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\left(\n",
    "\\partial_{\\mathbf p_i} \\boldsymbol\\Sigma^{-1}\n",
    "\\right)\n",
    "\\boldsymbol\\Sigma\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\partial_{\\mathbf p_i} \\left[\\boldsymbol\\Lambda + \\operatorname{diag}[\\mathbf p_i] \\right]\n",
    "\\boldsymbol\\Sigma\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\Sigma\n",
    "\\mathbf J_{ii}\n",
    "\\boldsymbol\\Sigma\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\boldsymbol\\sigma_i\n",
    "\\boldsymbol\\sigma_i^\\top\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "where $\\boldsymbol\\sigma_i$ is the $i^{\\text{th}}$ row of $\\boldsymbol\\Sigma$ and $\\mathbf J_{ii}$ is a matrix which is zero evewhere, except for at index $(i,i)$, where it is $1$. \n",
    "\n",
    "$$\n",
    "%-\\tfrac 1 2 \\left(\\boldsymbol\\Sigma\\boldsymbol\\Lambda\\boldsymbol\\Sigma-\\boldsymbol\\Sigma\\right)\\\\&=\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-processing",
   "metadata": {},
   "source": [
    "According to The Matrix Cookbook, if $\\boldsymbol\\Sigma(\\theta_i)$ is a function of a parameter $\\theta_i$, then the chain rule is: \n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\theta_i} = \n",
    "\\left<\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol\\Sigma}\n",
    ",\n",
    "\\frac{\\partial\\boldsymbol\\Sigma}{\\partial \\theta_i}\n",
    "\\right>\n",
    "=\n",
    "\\operatorname{tr}\\left[\n",
    "\\left(\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol\\Sigma}\n",
    "\\right)^\\top\n",
    "\\frac{\\partial\\boldsymbol\\Sigma}{\\partial \\theta_i}\n",
    "\\right]\n",
    "=\n",
    "\\sum_{kl}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol\\Sigma_{kl}}\n",
    "\\frac{\\partial\\boldsymbol\\Sigma_{kl}}{\\partial \\theta_i}\n",
    "\\end{aligned}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-motion",
   "metadata": {},
   "source": [
    "therefore\n",
    "\n",
    "\\begin{equation}\\begin{aligned}\n",
    "\\partial_{\\mathbf p_i}\\mathcal D\n",
    "&=\n",
    "\\sum_{kl}\n",
    "[\\partial_{\\boldsymbol\\Sigma_{kl}} \\mathcal D]\n",
    "[\\partial_{\\mathbf p_i} \\boldsymbol\\Sigma_{kl}\n",
    "]\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{kl}\n",
    "\\left\\{\n",
    "-\\tfrac 1 2 \\operatorname{diag}\\left[\\mathbf p\\right]\n",
    "\\right\\}_{kl}\n",
    "\\left\\{\n",
    "-\n",
    "\\boldsymbol\\sigma_i\n",
    "\\boldsymbol\\sigma_i^\\top\n",
    "\\right\\}_{kl}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{kl}\n",
    "\\delta_{k=l} \\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "\\boldsymbol\\sigma_{il}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{k}\n",
    "\\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "\\boldsymbol\\sigma_{ik}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\sum_{k}\n",
    "\\mathbf p_k\n",
    "\\boldsymbol\\sigma_{ik}^2\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\mathbf p\n",
    "\\boldsymbol\\sigma_i^{\\circ 2}\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\left\\{\n",
    "\\mathbf p\n",
    "\\boldsymbol\\Sigma^{\\circ 2}\n",
    "\\right\\}_i\n",
    "\\\\\n",
    "&=\n",
    "\\tfrac 1 2 \n",
    "\\left\\{\n",
    "\\operatorname{diag}\\left[\n",
    "\\boldsymbol\\Sigma\n",
    "\\operatorname{diag}\\left[\n",
    "\\mathbf p\n",
    "\\right]\n",
    "\\boldsymbol\\Sigma\n",
    "\\right]\n",
    "\\right\\}_i\n",
    "\\end{aligned}\\end{equation}\n",
    "\n",
    "where $(\\cdot)^{\\circ 2}$ denotes the element-wise square of a vector or matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-fiber",
   "metadata": {},
   "source": [
    "The Hessian-vector product is cumbersome, since each term in the expression $\\boldsymbol\\Sigma\\left(\\operatorname{diag}\\left[\\mathbf p\\right]\\right)\n",
    "\\boldsymbol\\Sigma$ depends on $\\mathbf p$. In the case of the log-linear Poisson GLM, the gradient $\\eqref{jp}$ simplifies further and optimization becomes tractable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
